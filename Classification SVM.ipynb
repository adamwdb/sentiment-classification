{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ganti nama file dari hasil tahap normalisasi sebelumnya\n",
    "file = pd.read_csv('data_OUT_TOTAL.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ganti nama file_input dengan file .txt file asli dari hasil preprocessing\n",
    "file_input = \"data_TOTAL.txt\"\n",
    "file_txt = open(file_input, mode='r', encoding='utf-8')\n",
    "text = file_txt.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['tweet_id'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code untuk mencari fitur POS\n",
    "list_feature = []\n",
    "\n",
    "for idx, t in enumerate(text):\n",
    "    selected = file[file['tweet_id'] == idx]\n",
    "    list_tweet = []\n",
    "    \n",
    "    if len(selected.index) > 0:\n",
    "        for a, tweet in selected.iterrows():\n",
    "            feature = float(tweet['normalize_weight'])\n",
    "            list_tweet.append(feature)\n",
    "            \n",
    "            \n",
    "        if len(selected.index) < pd.value_counts(file['tweet_id']).max():\n",
    "            b = len(selected.index)\n",
    "            while b < pd.value_counts(file['tweet_id']).max():\n",
    "                feature = float(0)\n",
    "                b += 1\n",
    "                list_tweet.append(feature)\n",
    "\n",
    "        \n",
    "    elif len(selected.index) == 0:\n",
    "        i = 0\n",
    "        while i < pd.value_counts(file['tweet_id']).max():\n",
    "            feature = float(0)\n",
    "            list_tweet.append(feature)\n",
    "            i += 1\n",
    "            \n",
    "    list_feature.append(list_tweet)\n",
    "    \n",
    "data = np.array(list_feature)\n",
    "#data = sparse.csr_matrix(sX)\n",
    "\n",
    "# format dimenasi yang didapat menggunakan fitur POS\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mencari label tweetnya dari file .txt hasil preprocessing\n",
    "lt = []\n",
    "\n",
    "for i,t in enumerate(text):\n",
    "    a = t.split(',')\n",
    "    label_y = a[1]\n",
    "    label_y = label_y.split('\\n')\n",
    "    lt.append(label_y[0])\n",
    "\n",
    "label_target = np.array(lt)\n",
    "# label_target = sparse.csr_matrix(sY.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code untuk mencari TERM PRESENCE\n",
    "tp = []\n",
    "df = file.drop_duplicates(subset=['term'])\n",
    "\n",
    "print(len(df))\n",
    "print(pd.value_counts(file['tweet_id']).max())\n",
    "\n",
    "for j in text:\n",
    "    tt = []\n",
    "    tmp = j.split(',')\n",
    "    tmp = tmp[0].split()\n",
    "    for k in range(len(df['term'])):\n",
    "        dd = df.iloc[k]['term']\n",
    "        #print(dd)\n",
    "        if any(dd in s for s in tmp):\n",
    "            tt.append(float(1))\n",
    "        else:\n",
    "            tt.append(float(0))\n",
    "    \n",
    "    tp.append(tt)\n",
    "\n",
    "term_presence = np.array(tp)\n",
    "\n",
    "print(len(term_presence))\n",
    "print(term_presence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code untuk menggabungkan fitur TERM PRESENCE dan POS Tagging\n",
    "data_total = np.column_stack([data, term_presence])\n",
    "\n",
    "print(data_total.shape)\n",
    "print(data_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk menampilkan confussion matrix pada hasil klasifikasi\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    #if normalize:\n",
    "    #    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    #    print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# code untuk Intelligent Model Selection\n",
    "SVCclassification_1 = svm.SVC(kernel='rbf')\n",
    "SVCclassification_2 = svm.SVC(kernel='rbf')\n",
    "\n",
    "# tunning nilai K untuk membagi jumlah dataset menjadi traning, testing, dan evaluasi\n",
    "skf = KFold(n_splits=10)\n",
    "target_names = ['Positif','Negatif']\n",
    "\n",
    "itter = 1\n",
    "ltt = []\n",
    "dtt = []\n",
    "best_model_tt = []\n",
    "label_best_model_tt = []\n",
    "evdtt = []\n",
    "evltt = []\n",
    "s = []\n",
    "r = []\n",
    "p = []\n",
    "f1 = []\n",
    "\n",
    "for train_test_index, validation_index in skf.split(data_total, label_target):\n",
    "    \n",
    "    ltt.clear()\n",
    "    dtt.clear()\n",
    "    evdtt.clear()\n",
    "    evltt.clear()\n",
    "    \n",
    "    # mencari label\n",
    "    #print(validation_index)\n",
    "    for i in train_test_index:\n",
    "        ltt.append(label_target[i])\n",
    "        dtt.append(data_total[i])\n",
    "        \n",
    "    label_testing_test = np.array(ltt)\n",
    "    data_testing_test = np.array(dtt)\n",
    "    \n",
    "    # find best model in second cross validation with 90% data\n",
    "    score2 = cross_val_score(SVCclassification_1, data_testing_test, label_testing_test, cv=skf)\n",
    "    \n",
    "    #print all score from second CV\n",
    "    #print(\"this is score : \", score2)\n",
    "    \n",
    "    # find index best score from second cross validation\n",
    "    best_score_index = np.argmax(score2)\n",
    "    \n",
    "    #print best index model\n",
    "    #print(\"this is best model index : \",best_score_index)\n",
    "    \n",
    "    counter = 0\n",
    "    for f_index,g_index in skf.split(data_testing_test,label_testing_test):\n",
    "        if counter == best_score_index:\n",
    "            best_model_tt.clear()\n",
    "            label_best_model_tt.clear()\n",
    "            for c in f_index:\n",
    "                best_model_tt.append(data_testing_test[c])\n",
    "                label_best_model_tt.append(label_testing_test[c])       \n",
    "            best_model = np.array(best_model_tt)    \n",
    "            best_model_label = np.array(label_best_model_tt)\n",
    "            break\n",
    "        counter += 1 \n",
    "    \n",
    "    # save evaluation test\n",
    "    for j in validation_index:\n",
    "        evltt.append(label_target[j])\n",
    "        evdtt.append(data_total[j])\n",
    "    \n",
    "    evaluation_datatest = np.array(evdtt)\n",
    "    evaluation_label_datatest = np.array(evltt)\n",
    "    \n",
    "    # fit BEST model with evaluation\n",
    "    SVCclassification_2.fit(best_model, best_model_label)    \n",
    "    \n",
    "    #print final accuracy (done with evalution set)\n",
    "    print(\"Best Final Accuracy on %0.0f CV: %0.3f\" % (itter, SVCclassification_2.score(evaluation_datatest, evaluation_label_datatest)))\n",
    "    itter += 1\n",
    "    \n",
    "    y_predic = SVCclassification_2.predict(evaluation_datatest)\n",
    "    #print(y_predic)\n",
    "    #print(evaluation_label_datatest)\n",
    "    \n",
    "    print(classification_report(evaluation_label_datatest, y_predic, target_names=target_names))\n",
    "    #print(confusion_matrix(evaluation_label_datatest, y_predic))\n",
    "    \n",
    "    # menghitung confusion matrix\n",
    "    cnf_matrix = confusion_matrix(evaluation_label_datatest, y_predic)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # menampilkan Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=target_names,title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    \n",
    "    # menyimpan hasil score , precision, recall, f1-score pada setiap iterasi\n",
    "    s.append(SVCclassification_2.score(evaluation_datatest, evaluation_label_datatest))\n",
    "    p.append(precision_score(evaluation_label_datatest, y_predic, pos_label='1'))\n",
    "    r.append(recall_score(evaluation_label_datatest, y_predic, pos_label='1'))\n",
    "    f1.append(f1_score(evaluation_label_datatest, y_predic, pos_label='1'))\n",
    "    \n",
    "save_accuracy_np = np.array(s)\n",
    "#print(\"acc : \",save_accuracy_np)\n",
    "\n",
    "save_recall_np = np.array(r)\n",
    "#print(\"recall : \" ,save_recall_np)\n",
    "\n",
    "save_precision_np = np.array(p)\n",
    "#print(\"precision : \", save_precision_np)\n",
    "\n",
    "save_f1_np = np.array(f1)\n",
    "#print(\"f1 : \" , save_f1_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (np.average(save_accuracy_np), np.std(save_accuracy_np)*2)) \n",
    "print(\"Precision: %0.3f\" % (np.average(save_precision_np))) \n",
    "print(\"Recall: %0.3f\" % (np.average(save_recall_np))) \n",
    "print(\"f1 score: %0.3f\" % (np.average(save_f1_np)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation normal tanpa IMS\n",
    "# target_names = ['negatif', 'positif']\n",
    "# clf = svm.SVC(kernel='linear')\n",
    "# skf = KFold(n_splits=6)\n",
    "# scores = cross_val_score(clf, data_total, label_target, cv=skf)\n",
    "# predicted = cross_val_predict(clf, data_total, label_target, cv=skf)\n",
    "\n",
    "# print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std()*2))\n",
    "\n",
    "# print(classification_report(label_target, predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(x=[\"IMS\", \"normal CV\"], y=[np.average(save_accuracy_np), scores.mean()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
