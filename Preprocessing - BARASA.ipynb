{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import barasa as ba\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import re\n",
    "import json\n",
    "import numpy\n",
    "from tqdm import *\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ganti variabel ftweet dengan lokasi file hasil crawling twitter\n",
    "ganti variabel stopword untuk lokasi stopword yang digunakan\n",
    "ganti variabel slang_dic untuk lokasi slang kata-kata yang perlu dihapus\n",
    "'''\n",
    "\n",
    "Ftweets=\"Tweets/XL/mentah_negatif.txt\"\n",
    "stopword=\"stopwords_id.txt\"\n",
    "slang_dic=\"slang.dic\"\n",
    "\n",
    "# bentuk kata yang ingin dihapus melalui regex\n",
    "regexTokenizer = RegexpTokenizer(r'\\w+')\n",
    "regexPattern1 = re.compile(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)')\n",
    "regexPattern2 = re.compile(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*')\n",
    "regexPattern3 = re.compile(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\")\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# membuka tweet file\n",
    "ftweet = io.open(Ftweets, mode='r', encoding='utf-8')\n",
    "rawdata = ftweet.readlines()\n",
    "ftweet.close()\n",
    "\n",
    "# membuka slang dic\n",
    "slang = io.open(slang_dic, mode='r', encoding='utf-8')\n",
    "slangDic = slang.readlines()\n",
    "slang.close()\n",
    "slangDic = [t.strip('\\n') for t in slangDic]\n",
    "slangDic = dict((t.split(\":\")[0],t.split(\":\")[1]) for t in slangDic)\n",
    "\n",
    "# membuka stopword\n",
    "sw = io.open(stopword, encoding='utf-8', mode='r')\n",
    "stop = sw.readlines()\n",
    "stop_ind = [t.strip() for t in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in rawdata:\n",
    "    tweet = status\n",
    "    tweet = re.sub(regexPattern1,' ',tweet)\n",
    "    tweet = re.sub(regexPattern2, ' ', tweet)\n",
    "    tweet = re.sub(regexPattern3, ' ', tweet)\n",
    "    \n",
    "    # code untuk menghapus kata-kata pada file slang_dic\n",
    "    for slang, formal in slangDic.items():\n",
    "        tweet = tweet.replace(slang,formal)\n",
    "        \n",
    "    tweet = regexTokenizer.tokenize(tweet)\n",
    "    \n",
    "    # code untuk menghapus kata-kata pada stopwords\n",
    "    tweet_stop = [word for word in tweet if word not in stop_ind]\n",
    "    \n",
    "    if tweet_stop:  \n",
    "        clean_tweet = ' '.join(tweet_stop)\n",
    "        stemmedTweet = stemmer.stem(str(clean_tweet))\n",
    "        \n",
    "        # hasil.write(cleanTweet + '\\n')\n",
    "        print(stemmedTweet)\n",
    "        \n",
    "# hasil print tweet yang telah dipreprocessing di copy pada sebuah file dengan ekstensi .txt\n",
    "# untuk format file .txt tweet yang telah didapat disimpan dengan dataset format (tweet,kelas tweet)\n",
    "# kelas tweet diisi dengan angka 1 untuk tweet positif dan 0 untuk tweet negatif\n",
    "# contoh : 'kurang bagus sinyal daerah,0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLDIC_ALL.csv\n",
      "ALLDIC_IM3.csv\n",
      "ALLDIC_SINTESIS.csv\n",
      "ALLDIC_TELKOMSEL.csv\n",
      "ALLDIC_XL.csv\n",
      "Classification SVM.ipynb\n",
      "Normalize SentiMI.ipynb\n",
      "OUT_ALL.csv\n",
      "OUT_IM3.csv\n",
      "OUT_SINTESIS.csv\n",
      "OUT_TELKOMSEL.csv\n",
      "OUT_XL.csv\n",
      "PETUNJUK TA.txt\n",
      "Preprocessing - BARASA.ipynb\n",
      "REVISI_IM3.txt\n",
      "REVISI_SINTESIS.txt\n",
      "REVISI_TELKOMSEL.txt\n",
      "REVISI_XL.txt\n",
      "SentiWordNet.txt\n",
      "Sentiwordnet Dic.ipynb\n",
      "Sintesis_negatif.txt\n",
      "Sintesis_positif.txt\n",
      "Tweets\n",
      "barasa copy.txt\n",
      "barasa.csv\n",
      "craw1\n",
      "craw2\n",
      "data\n",
      "data_DIC_TOTAL.csv\n",
      "data_OUT_TOTAL.csv\n",
      "data_TOTAL.txt\n",
      "labeledTrainData2.tsv\n",
      "normalize_sentiMI.csv\n",
      "pic1.png\n",
      "pic2.png\n",
      "pic3.png\n",
      "pic4.png\n",
      "pic5.png\n",
      "positif.csv\n",
      "python file\n",
      "slang.dic\n",
      "stopwords_eng.txt\n",
      "stopwords_id.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
