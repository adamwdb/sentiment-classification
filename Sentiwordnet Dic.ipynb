{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "from spacy.lang.id import Indonesian\n",
    "from itertools import zip_longest\n",
    "nlp_id_spacy = Indonesian()\n",
    "pos_term = []\n",
    "pos_term_neg = []\n",
    "csv.register_dialect('myDialect', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lokasi untuk file barasa sebagai sumber lexical pada tugas akhir\n",
    "df = pd.read_csv('barasa.csv', delimiter='\\t', index_col=False)\n",
    "\n",
    "# file_input sebagai variabel sumber file tweet yang telah didapat dari proses preprocessing\n",
    "file_input = \"data_ALL.txt\"\n",
    "file = open(file_input, mode='r', encoding='utf-8')\n",
    "text = file.readlines()\n",
    "\n",
    "# variabel file untuk menyimpan hasil proses sentiwordnet. Ganti nama file jika dibutuhkan\n",
    "file = \"data_DIC_TOTAL.csv\"\n",
    "senti_dic = open(file, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk memanggil kamus kelas kata (POS Tagging Bahasa Indonesia). ganti file sesuai dengan lokasi kamus kata\n",
    "def loadPos_id(file = 'data/kata_dasar.txt'):\n",
    "    kata_pos = {}\n",
    "    df=open(file,\"r\",encoding=\"utf-8\", errors='replace')\n",
    "    data=df.readlines();df.close()\n",
    "    \n",
    "    for line in data:\n",
    "        d = line.split()\n",
    "        kata = d[0].strip()\n",
    "        \n",
    "        pos = d[-1].strip().replace(\"(\",'').replace(')','')\n",
    "        kata_pos[kata] = pos\n",
    "        \n",
    "    return kata_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['lemma'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk memberikan kelas kata pada setiap kata tweet yang telah didapat\n",
    "kata_pos = loadPos_id()\n",
    "pos_term.clear()\n",
    "\n",
    "for idx, texts in enumerate(text):\n",
    "    tweet = texts.split(\",\")\n",
    "    label = tweet[1].split(\"\\n\")\n",
    "    wordList = tweet[0].split()\n",
    "    \n",
    "    for term in wordList:\n",
    "        try:\n",
    "            pos_term.append([idx, term , kata_pos[term], label[0]]) \n",
    "        except:\n",
    "            pos_term.append([idx, term , 'n', label[0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d = []\n",
    "q = []\n",
    "tweet_id = 'tweet_id'\n",
    "term = 'term'\n",
    "pos = 'pos'\n",
    "pos_fre = 'pos_fre'\n",
    "neg_fre = 'neg_fre'\n",
    "label = 'label'\n",
    "q.append(tweet_id)\n",
    "q.append(term)\n",
    "q.append(pos)\n",
    "q.append(pos_fre)\n",
    "q.append(neg_fre)\n",
    "q.append(label)\n",
    "\n",
    "# file csv untuk menyimpan hasil sentiwordnet \n",
    "csv_save = csv.writer(senti_dic,dialect='myDialect')\n",
    "csv_save.writerow(q)\n",
    "\n",
    "for itter in range(len(pos_term)):\n",
    "    count_positif = 0\n",
    "    count_negatif = 0\n",
    "\n",
    "    item = df[df['lemma']==pos_term[itter][1]]\n",
    "    if pos_term[itter][2] == \"adj\":\n",
    "        pos_synset = \"a\"\n",
    "    elif pos_term[itter][2] == \"n\":\n",
    "        pos_synset = \"n\"\n",
    "    elif pos_term[itter][2] == \"v\":\n",
    "        pos_synset = \"v\"\n",
    "    elif pos_term[itter][2] == \"k\":\n",
    "        pos_synset = \"r\"\n",
    "        \n",
    "    item = item[item['language'].str.contains(\"I|B\")]\n",
    "    item = item[item['synset'].str.contains(pos_synset)]\n",
    "    item = item[item['goodness'].str.contains(\"Y|O|M\")]\n",
    "    item = item[item['PosScore']-item['NegScore'] != 0]\n",
    "    \n",
    "    item_POS = item[item['PosScore']-item['NegScore'] > 0]\n",
    "    item_NEG = item[item['PosScore']-item['NegScore'] < 0]\n",
    "\n",
    "    if len(item.index) > 0:\n",
    "        if len(item_POS) > 0:\n",
    "            count_positif += len(item_POS)   \n",
    "        if len(item_NEG) > 0:\n",
    "            count_negatif += len(item_NEG)\n",
    "        \n",
    "        # save csv\n",
    "        d.append(pos_term[itter][0])\n",
    "        d.append(pos_term[itter][1])\n",
    "        d.append(pos_term[itter][2])\n",
    "        d.append(count_positif)\n",
    "        d.append(count_negatif)\n",
    "        d.append(pos_term[itter][3])\n",
    "        \n",
    "        csv_save = csv.writer(senti_dic,dialect='myDialect')\n",
    "        csv_save.writerow(d)\n",
    "        \n",
    "    d.clear()   \n",
    "    \n",
    "senti_dic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
